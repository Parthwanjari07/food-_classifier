{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: torch in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.0+cu118)\n",
      "Requirement already satisfied: gradio in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.25.2)\n",
      "Requirement already satisfied: safetensors in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (70.1.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (4.4.0)\n",
      "Requirement already satisfied: fastapi<1.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.110.3)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: packaging in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (2.5.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.6.9)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio) (0.30.6)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi<1.0->gradio) (0.37.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.24.1->gradio) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\prerdiator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm torch torchvision gradio datasets pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRERDIATOR\\AppData\\Local\\Temp\\ipykernel_11496\\2029054124.py:101: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pretrained_weights = torch.load(MODEL_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.6183\n",
      "Validation Accuracy: 57.45%\n",
      "Epoch 2/20, Loss: 0.2503\n",
      "Validation Accuracy: 62.91%\n",
      "Epoch 3/20, Loss: 0.0447\n",
      "Validation Accuracy: 65.09%\n",
      "Epoch 4/20, Loss: 0.0215\n",
      "Validation Accuracy: 64.73%\n",
      "Epoch 5/20, Loss: 0.0140\n",
      "Validation Accuracy: 66.91%\n",
      "Epoch 6/20, Loss: 0.0094\n",
      "Validation Accuracy: 65.45%\n",
      "Epoch 7/20, Loss: 0.0071\n",
      "Validation Accuracy: 66.55%\n",
      "Epoch 8/20, Loss: 0.0055\n",
      "Validation Accuracy: 66.91%\n",
      "Epoch 9/20, Loss: 0.0046\n",
      "Validation Accuracy: 65.82%\n",
      "Epoch 10/20, Loss: 0.0040\n",
      "Validation Accuracy: 67.64%\n",
      "Epoch 11/20, Loss: 0.0033\n",
      "Validation Accuracy: 68.00%\n",
      "Epoch 12/20, Loss: 0.0029\n",
      "Validation Accuracy: 67.64%\n",
      "Epoch 13/20, Loss: 0.0033\n",
      "Validation Accuracy: 68.00%\n",
      "Epoch 14/20, Loss: 0.0025\n",
      "Validation Accuracy: 66.55%\n",
      "Epoch 15/20, Loss: 0.0023\n",
      "Validation Accuracy: 66.91%\n",
      "Epoch 16/20, Loss: 0.0019\n",
      "Validation Accuracy: 67.64%\n",
      "Epoch 17/20, Loss: 0.0017\n",
      "Validation Accuracy: 66.55%\n",
      "Epoch 18/20, Loss: 0.0015\n",
      "Validation Accuracy: 66.18%\n",
      "Epoch 19/20, Loss: 0.0014\n",
      "Validation Accuracy: 66.55%\n",
      "Epoch 20/20, Loss: 0.0013\n",
      "Validation Accuracy: 66.18%\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Set the paths for your local data and model\n",
    "DATA_DIR = \"food _classifier/DATA_DIR\"\n",
    "MODEL_PATH = \"food _classifier/food_classifier_cnn_lstm.pth\"\n",
    "NEW_MODEL_PATH = \"food_classifier_cnn_lstm.pth\"\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = load_from_disk(os.path.join(DATA_DIR, \"train\"))\n",
    "val_dataset = load_from_disk(os.path.join(DATA_DIR, \"validation\"))\n",
    "\n",
    "# Custom Dataset class\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        image = item['image']\n",
    "    \n",
    "    # If the image is not already in RGB, convert it\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "    \n",
    "        label = item['label']\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, label\n",
    "\n",
    "# Define the model\n",
    "class FoodClassifierCNNLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=256, num_layers=2):\n",
    "        super(FoodClassifierCNNLSTM, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n",
    "        \n",
    "        self.lstm = nn.LSTM(2048, hidden_size, num_layers, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)  # Added LayerNorm\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        cnn_features = self.resnet(x)  # Shape: (batch_size, 2048, H, W)\n",
    "        cnn_features = cnn_features.view(batch_size, 2048, -1).permute(0, 2, 1)  # Shape: (batch_size, seq_len, 2048)\n",
    "        lstm_out, _ = self.lstm(cnn_features)  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        lstm_out = self.layer_norm(lstm_out)   # Apply LayerNorm\n",
    "        lstm_out = lstm_out[:, -1, :]         # Use the last output of LSTM\n",
    "        output = self.fc(lstm_out)            # Final classification layer\n",
    "        return output\n",
    "\n",
    "# Temporary model for loading pre-trained weights\n",
    "class TempFoodClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TempFoodClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# Set up data transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FoodDataset(train_dataset, transform=data_transforms)\n",
    "val_dataset = FoodDataset(val_dataset, transform=data_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = len(set(train_dataset.hf_dataset['label']))\n",
    "model = FoodClassifierCNNLSTM(num_classes)\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the previously trained weights\n",
    "pretrained_weights = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "# Create a temporary model with the same architecture as the one used for training\n",
    "temp_model = TempFoodClassifier(num_classes)\n",
    "temp_model.load_state_dict(pretrained_weights, strict=False)\n",
    "\n",
    "\n",
    "# Copy weights from the temporary model to the ResNet part of our new model\n",
    "model.resnet.load_state_dict(temp_model.resnet.state_dict(), strict=False)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower learning rate for fine-tuning\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the new model\n",
    "torch.save(model.state_dict(), NEW_MODEL_PATH)\n",
    "\n",
    "# Function to predict\n",
    "def predict(model, image_path):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = data_transforms(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    return train_dataset.hf_dataset.features['label'].int2str(predicted.item())\n",
    "\n",
    "# Example usage:\n",
    "# result = predict(model, '/path/to/test/image.jpg')\n",
    "# print(f\"Predicted food category: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PRERDIATOR\\AppData\\Local\\Temp\\ipykernel_11496\\106446845.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=torch.device('cuda'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: Error(s) in loading state_dict for FoodClassifier:\n",
      "\tMissing key(s) in state_dict: \"resnet.conv1.weight\", \"resnet.bn1.weight\", \"resnet.bn1.bias\", \"resnet.bn1.running_mean\", \"resnet.bn1.running_var\", \"resnet.layer1.0.conv1.weight\", \"resnet.layer1.0.bn1.weight\", \"resnet.layer1.0.bn1.bias\", \"resnet.layer1.0.bn1.running_mean\", \"resnet.layer1.0.bn1.running_var\", \"resnet.layer1.0.conv2.weight\", \"resnet.layer1.0.bn2.weight\", \"resnet.layer1.0.bn2.bias\", \"resnet.layer1.0.bn2.running_mean\", \"resnet.layer1.0.bn2.running_var\", \"resnet.layer1.0.conv3.weight\", \"resnet.layer1.0.bn3.weight\", \"resnet.layer1.0.bn3.bias\", \"resnet.layer1.0.bn3.running_mean\", \"resnet.layer1.0.bn3.running_var\", \"resnet.layer1.0.downsample.0.weight\", \"resnet.layer1.0.downsample.1.weight\", \"resnet.layer1.0.downsample.1.bias\", \"resnet.layer1.0.downsample.1.running_mean\", \"resnet.layer1.0.downsample.1.running_var\", \"resnet.layer1.1.conv1.weight\", \"resnet.layer1.1.bn1.weight\", \"resnet.layer1.1.bn1.bias\", \"resnet.layer1.1.bn1.running_mean\", \"resnet.layer1.1.bn1.running_var\", \"resnet.layer1.1.conv2.weight\", \"resnet.layer1.1.bn2.weight\", \"resnet.layer1.1.bn2.bias\", \"resnet.layer1.1.bn2.running_mean\", \"resnet.layer1.1.bn2.running_var\", \"resnet.layer1.1.conv3.weight\", \"resnet.layer1.1.bn3.weight\", \"resnet.layer1.1.bn3.bias\", \"resnet.layer1.1.bn3.running_mean\", \"resnet.layer1.1.bn3.running_var\", \"resnet.layer1.2.conv1.weight\", \"resnet.layer1.2.bn1.weight\", \"resnet.layer1.2.bn1.bias\", \"resnet.layer1.2.bn1.running_mean\", \"resnet.layer1.2.bn1.running_var\", \"resnet.layer1.2.conv2.weight\", \"resnet.layer1.2.bn2.weight\", \"resnet.layer1.2.bn2.bias\", \"resnet.layer1.2.bn2.running_mean\", \"resnet.layer1.2.bn2.running_var\", \"resnet.layer1.2.conv3.weight\", \"resnet.layer1.2.bn3.weight\", \"resnet.layer1.2.bn3.bias\", \"resnet.layer1.2.bn3.running_mean\", \"resnet.layer1.2.bn3.running_var\", \"resnet.layer2.0.conv1.weight\", \"resnet.layer2.0.bn1.weight\", \"resnet.layer2.0.bn1.bias\", \"resnet.layer2.0.bn1.running_mean\", \"resnet.layer2.0.bn1.running_var\", \"resnet.layer2.0.conv2.weight\", \"resnet.layer2.0.bn2.weight\", \"resnet.layer2.0.bn2.bias\", \"resnet.layer2.0.bn2.running_mean\", \"resnet.layer2.0.bn2.running_var\", \"resnet.layer2.0.conv3.weight\", \"resnet.layer2.0.bn3.weight\", \"resnet.layer2.0.bn3.bias\", \"resnet.layer2.0.bn3.running_mean\", \"resnet.layer2.0.bn3.running_var\", \"resnet.layer2.0.downsample.0.weight\", \"resnet.layer2.0.downsample.1.weight\", \"resnet.layer2.0.downsample.1.bias\", \"resnet.layer2.0.downsample.1.running_mean\", \"resnet.layer2.0.downsample.1.running_var\", \"resnet.layer2.1.conv1.weight\", \"resnet.layer2.1.bn1.weight\", \"resnet.layer2.1.bn1.bias\", \"resnet.layer2.1.bn1.running_mean\", \"resnet.layer2.1.bn1.running_var\", \"resnet.layer2.1.conv2.weight\", \"resnet.layer2.1.bn2.weight\", \"resnet.layer2.1.bn2.bias\", \"resnet.layer2.1.bn2.running_mean\", \"resnet.layer2.1.bn2.running_var\", \"resnet.layer2.1.conv3.weight\", \"resnet.layer2.1.bn3.weight\", \"resnet.layer2.1.bn3.bias\", \"resnet.layer2.1.bn3.running_mean\", \"resnet.layer2.1.bn3.running_var\", \"resnet.layer2.2.conv1.weight\", \"resnet.layer2.2.bn1.weight\", \"resnet.layer2.2.bn1.bias\", \"resnet.layer2.2.bn1.running_mean\", \"resnet.layer2.2.bn1.running_var\", \"resnet.layer2.2.conv2.weight\", \"resnet.layer2.2.bn2.weight\", \"resnet.layer2.2.bn2.bias\", \"resnet.layer2.2.bn2.running_mean\", \"resnet.layer2.2.bn2.running_var\", \"resnet.layer2.2.conv3.weight\", \"resnet.layer2.2.bn3.weight\", \"resnet.layer2.2.bn3.bias\", \"resnet.layer2.2.bn3.running_mean\", \"resnet.layer2.2.bn3.running_var\", \"resnet.layer2.3.conv1.weight\", \"resnet.layer2.3.bn1.weight\", \"resnet.layer2.3.bn1.bias\", \"resnet.layer2.3.bn1.running_mean\", \"resnet.layer2.3.bn1.running_var\", \"resnet.layer2.3.conv2.weight\", \"resnet.layer2.3.bn2.weight\", \"resnet.layer2.3.bn2.bias\", \"resnet.layer2.3.bn2.running_mean\", \"resnet.layer2.3.bn2.running_var\", \"resnet.layer2.3.conv3.weight\", \"resnet.layer2.3.bn3.weight\", \"resnet.layer2.3.bn3.bias\", \"resnet.layer2.3.bn3.running_mean\", \"resnet.layer2.3.bn3.running_var\", \"resnet.layer3.0.conv1.weight\", \"resnet.layer3.0.bn1.weight\", \"resnet.layer3.0.bn1.bias\", \"resnet.layer3.0.bn1.running_mean\", \"resnet.layer3.0.bn1.running_var\", \"resnet.layer3.0.conv2.weight\", \"resnet.layer3.0.bn2.weight\", \"resnet.layer3.0.bn2.bias\", \"resnet.layer3.0.bn2.running_mean\", \"resnet.layer3.0.bn2.running_var\", \"resnet.layer3.0.conv3.weight\", \"resnet.layer3.0.bn3.weight\", \"resnet.layer3.0.bn3.bias\", \"resnet.layer3.0.bn3.running_mean\", \"resnet.layer3.0.bn3.running_var\", \"resnet.layer3.0.downsample.0.weight\", \"resnet.layer3.0.downsample.1.weight\", \"resnet.layer3.0.downsample.1.bias\", \"resnet.layer3.0.downsample.1.running_mean\", \"resnet.layer3.0.downsample.1.running_var\", \"resnet.layer3.1.conv1.weight\", \"resnet.layer3.1.bn1.weight\", \"resnet.layer3.1.bn1.bias\", \"resnet.layer3.1.bn1.running_mean\", \"resnet.layer3.1.bn1.running_var\", \"resnet.layer3.1.conv2.weight\", \"resnet.layer3.1.bn2.weight\", \"resnet.layer3.1.bn2.bias\", \"resnet.layer3.1.bn2.running_mean\", \"resnet.layer3.1.bn2.running_var\", \"resnet.layer3.1.conv3.weight\", \"resnet.layer3.1.bn3.weight\", \"resnet.layer3.1.bn3.bias\", \"resnet.layer3.1.bn3.running_mean\", \"resnet.layer3.1.bn3.running_var\", \"resnet.layer3.2.conv1.weight\", \"resnet.layer3.2.bn1.weight\", \"resnet.layer3.2.bn1.bias\", \"resnet.layer3.2.bn1.running_mean\", \"resnet.layer3.2.bn1.running_var\", \"resnet.layer3.2.conv2.weight\", \"resnet.layer3.2.bn2.weight\", \"resnet.layer3.2.bn2.bias\", \"resnet.layer3.2.bn2.running_mean\", \"resnet.layer3.2.bn2.running_var\", \"resnet.layer3.2.conv3.weight\", \"resnet.layer3.2.bn3.weight\", \"resnet.layer3.2.bn3.bias\", \"resnet.layer3.2.bn3.running_mean\", \"resnet.layer3.2.bn3.running_var\", \"resnet.layer3.3.conv1.weight\", \"resnet.layer3.3.bn1.weight\", \"resnet.layer3.3.bn1.bias\", \"resnet.layer3.3.bn1.running_mean\", \"resnet.layer3.3.bn1.running_var\", \"resnet.layer3.3.conv2.weight\", \"resnet.layer3.3.bn2.weight\", \"resnet.layer3.3.bn2.bias\", \"resnet.layer3.3.bn2.running_mean\", \"resnet.layer3.3.bn2.running_var\", \"resnet.layer3.3.conv3.weight\", \"resnet.layer3.3.bn3.weight\", \"resnet.layer3.3.bn3.bias\", \"resnet.layer3.3.bn3.running_mean\", \"resnet.layer3.3.bn3.running_var\", \"resnet.layer3.4.conv1.weight\", \"resnet.layer3.4.bn1.weight\", \"resnet.layer3.4.bn1.bias\", \"resnet.layer3.4.bn1.running_mean\", \"resnet.layer3.4.bn1.running_var\", \"resnet.layer3.4.conv2.weight\", \"resnet.layer3.4.bn2.weight\", \"resnet.layer3.4.bn2.bias\", \"resnet.layer3.4.bn2.running_mean\", \"resnet.layer3.4.bn2.running_var\", \"resnet.layer3.4.conv3.weight\", \"resnet.layer3.4.bn3.weight\", \"resnet.layer3.4.bn3.bias\", \"resnet.layer3.4.bn3.running_mean\", \"resnet.layer3.4.bn3.running_var\", \"resnet.layer3.5.conv1.weight\", \"resnet.layer3.5.bn1.weight\", \"resnet.layer3.5.bn1.bias\", \"resnet.layer3.5.bn1.running_mean\", \"resnet.layer3.5.bn1.running_var\", \"resnet.layer3.5.conv2.weight\", \"resnet.layer3.5.bn2.weight\", \"resnet.layer3.5.bn2.bias\", \"resnet.layer3.5.bn2.running_mean\", \"resnet.layer3.5.bn2.running_var\", \"resnet.layer3.5.conv3.weight\", \"resnet.layer3.5.bn3.weight\", \"resnet.layer3.5.bn3.bias\", \"resnet.layer3.5.bn3.running_mean\", \"resnet.layer3.5.bn3.running_var\", \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.conv3.weight\", \"resnet.layer4.0.bn3.weight\", \"resnet.layer4.0.bn3.bias\", \"resnet.layer4.0.bn3.running_mean\", \"resnet.layer4.0.bn3.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\", \"resnet.layer4.1.conv3.weight\", \"resnet.layer4.1.bn3.weight\", \"resnet.layer4.1.bn3.bias\", \"resnet.layer4.1.bn3.running_mean\", \"resnet.layer4.1.bn3.running_var\", \"resnet.layer4.2.conv1.weight\", \"resnet.layer4.2.bn1.weight\", \"resnet.layer4.2.bn1.bias\", \"resnet.layer4.2.bn1.running_mean\", \"resnet.layer4.2.bn1.running_var\", \"resnet.layer4.2.conv2.weight\", \"resnet.layer4.2.bn2.weight\", \"resnet.layer4.2.bn2.bias\", \"resnet.layer4.2.bn2.running_mean\", \"resnet.layer4.2.bn2.running_var\", \"resnet.layer4.2.conv3.weight\", \"resnet.layer4.2.bn3.weight\", \"resnet.layer4.2.bn3.bias\", \"resnet.layer4.2.bn3.running_mean\", \"resnet.layer4.2.bn3.running_var\", \"resnet.fc.weight\", \"resnet.fc.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\", \"layer_norm.weight\", \"layer_norm.bias\", \"fc.weight\", \"fc.bias\", \"resnet.0.weight\", \"resnet.1.weight\", \"resnet.1.bias\", \"resnet.1.running_mean\", \"resnet.1.running_var\", \"resnet.1.num_batches_tracked\", \"resnet.4.0.conv1.weight\", \"resnet.4.0.bn1.weight\", \"resnet.4.0.bn1.bias\", \"resnet.4.0.bn1.running_mean\", \"resnet.4.0.bn1.running_var\", \"resnet.4.0.bn1.num_batches_tracked\", \"resnet.4.0.conv2.weight\", \"resnet.4.0.bn2.weight\", \"resnet.4.0.bn2.bias\", \"resnet.4.0.bn2.running_mean\", \"resnet.4.0.bn2.running_var\", \"resnet.4.0.bn2.num_batches_tracked\", \"resnet.4.0.conv3.weight\", \"resnet.4.0.bn3.weight\", \"resnet.4.0.bn3.bias\", \"resnet.4.0.bn3.running_mean\", \"resnet.4.0.bn3.running_var\", \"resnet.4.0.bn3.num_batches_tracked\", \"resnet.4.0.downsample.0.weight\", \"resnet.4.0.downsample.1.weight\", \"resnet.4.0.downsample.1.bias\", \"resnet.4.0.downsample.1.running_mean\", \"resnet.4.0.downsample.1.running_var\", \"resnet.4.0.downsample.1.num_batches_tracked\", \"resnet.4.1.conv1.weight\", \"resnet.4.1.bn1.weight\", \"resnet.4.1.bn1.bias\", \"resnet.4.1.bn1.running_mean\", \"resnet.4.1.bn1.running_var\", \"resnet.4.1.bn1.num_batches_tracked\", \"resnet.4.1.conv2.weight\", \"resnet.4.1.bn2.weight\", \"resnet.4.1.bn2.bias\", \"resnet.4.1.bn2.running_mean\", \"resnet.4.1.bn2.running_var\", \"resnet.4.1.bn2.num_batches_tracked\", \"resnet.4.1.conv3.weight\", \"resnet.4.1.bn3.weight\", \"resnet.4.1.bn3.bias\", \"resnet.4.1.bn3.running_mean\", \"resnet.4.1.bn3.running_var\", \"resnet.4.1.bn3.num_batches_tracked\", \"resnet.4.2.conv1.weight\", \"resnet.4.2.bn1.weight\", \"resnet.4.2.bn1.bias\", \"resnet.4.2.bn1.running_mean\", \"resnet.4.2.bn1.running_var\", \"resnet.4.2.bn1.num_batches_tracked\", \"resnet.4.2.conv2.weight\", \"resnet.4.2.bn2.weight\", \"resnet.4.2.bn2.bias\", \"resnet.4.2.bn2.running_mean\", \"resnet.4.2.bn2.running_var\", \"resnet.4.2.bn2.num_batches_tracked\", \"resnet.4.2.conv3.weight\", \"resnet.4.2.bn3.weight\", \"resnet.4.2.bn3.bias\", \"resnet.4.2.bn3.running_mean\", \"resnet.4.2.bn3.running_var\", \"resnet.4.2.bn3.num_batches_tracked\", \"resnet.5.0.conv1.weight\", \"resnet.5.0.bn1.weight\", \"resnet.5.0.bn1.bias\", \"resnet.5.0.bn1.running_mean\", \"resnet.5.0.bn1.running_var\", \"resnet.5.0.bn1.num_batches_tracked\", \"resnet.5.0.conv2.weight\", \"resnet.5.0.bn2.weight\", \"resnet.5.0.bn2.bias\", \"resnet.5.0.bn2.running_mean\", \"resnet.5.0.bn2.running_var\", \"resnet.5.0.bn2.num_batches_tracked\", \"resnet.5.0.conv3.weight\", \"resnet.5.0.bn3.weight\", \"resnet.5.0.bn3.bias\", \"resnet.5.0.bn3.running_mean\", \"resnet.5.0.bn3.running_var\", \"resnet.5.0.bn3.num_batches_tracked\", \"resnet.5.0.downsample.0.weight\", \"resnet.5.0.downsample.1.weight\", \"resnet.5.0.downsample.1.bias\", \"resnet.5.0.downsample.1.running_mean\", \"resnet.5.0.downsample.1.running_var\", \"resnet.5.0.downsample.1.num_batches_tracked\", \"resnet.5.1.conv1.weight\", \"resnet.5.1.bn1.weight\", \"resnet.5.1.bn1.bias\", \"resnet.5.1.bn1.running_mean\", \"resnet.5.1.bn1.running_var\", \"resnet.5.1.bn1.num_batches_tracked\", \"resnet.5.1.conv2.weight\", \"resnet.5.1.bn2.weight\", \"resnet.5.1.bn2.bias\", \"resnet.5.1.bn2.running_mean\", \"resnet.5.1.bn2.running_var\", \"resnet.5.1.bn2.num_batches_tracked\", \"resnet.5.1.conv3.weight\", \"resnet.5.1.bn3.weight\", \"resnet.5.1.bn3.bias\", \"resnet.5.1.bn3.running_mean\", \"resnet.5.1.bn3.running_var\", \"resnet.5.1.bn3.num_batches_tracked\", \"resnet.5.2.conv1.weight\", \"resnet.5.2.bn1.weight\", \"resnet.5.2.bn1.bias\", \"resnet.5.2.bn1.running_mean\", \"resnet.5.2.bn1.running_var\", \"resnet.5.2.bn1.num_batches_tracked\", \"resnet.5.2.conv2.weight\", \"resnet.5.2.bn2.weight\", \"resnet.5.2.bn2.bias\", \"resnet.5.2.bn2.running_mean\", \"resnet.5.2.bn2.running_var\", \"resnet.5.2.bn2.num_batches_tracked\", \"resnet.5.2.conv3.weight\", \"resnet.5.2.bn3.weight\", \"resnet.5.2.bn3.bias\", \"resnet.5.2.bn3.running_mean\", \"resnet.5.2.bn3.running_var\", \"resnet.5.2.bn3.num_batches_tracked\", \"resnet.5.3.conv1.weight\", \"resnet.5.3.bn1.weight\", \"resnet.5.3.bn1.bias\", \"resnet.5.3.bn1.running_mean\", \"resnet.5.3.bn1.running_var\", \"resnet.5.3.bn1.num_batches_tracked\", \"resnet.5.3.conv2.weight\", \"resnet.5.3.bn2.weight\", \"resnet.5.3.bn2.bias\", \"resnet.5.3.bn2.running_mean\", \"resnet.5.3.bn2.running_var\", \"resnet.5.3.bn2.num_batches_tracked\", \"resnet.5.3.conv3.weight\", \"resnet.5.3.bn3.weight\", \"resnet.5.3.bn3.bias\", \"resnet.5.3.bn3.running_mean\", \"resnet.5.3.bn3.running_var\", \"resnet.5.3.bn3.num_batches_tracked\", \"resnet.6.0.conv1.weight\", \"resnet.6.0.bn1.weight\", \"resnet.6.0.bn1.bias\", \"resnet.6.0.bn1.running_mean\", \"resnet.6.0.bn1.running_var\", \"resnet.6.0.bn1.num_batches_tracked\", \"resnet.6.0.conv2.weight\", \"resnet.6.0.bn2.weight\", \"resnet.6.0.bn2.bias\", \"resnet.6.0.bn2.running_mean\", \"resnet.6.0.bn2.running_var\", \"resnet.6.0.bn2.num_batches_tracked\", \"resnet.6.0.conv3.weight\", \"resnet.6.0.bn3.weight\", \"resnet.6.0.bn3.bias\", \"resnet.6.0.bn3.running_mean\", \"resnet.6.0.bn3.running_var\", \"resnet.6.0.bn3.num_batches_tracked\", \"resnet.6.0.downsample.0.weight\", \"resnet.6.0.downsample.1.weight\", \"resnet.6.0.downsample.1.bias\", \"resnet.6.0.downsample.1.running_mean\", \"resnet.6.0.downsample.1.running_var\", \"resnet.6.0.downsample.1.num_batches_tracked\", \"resnet.6.1.conv1.weight\", \"resnet.6.1.bn1.weight\", \"resnet.6.1.bn1.bias\", \"resnet.6.1.bn1.running_mean\", \"resnet.6.1.bn1.running_var\", \"resnet.6.1.bn1.num_batches_tracked\", \"resnet.6.1.conv2.weight\", \"resnet.6.1.bn2.weight\", \"resnet.6.1.bn2.bias\", \"resnet.6.1.bn2.running_mean\", \"resnet.6.1.bn2.running_var\", \"resnet.6.1.bn2.num_batches_tracked\", \"resnet.6.1.conv3.weight\", \"resnet.6.1.bn3.weight\", \"resnet.6.1.bn3.bias\", \"resnet.6.1.bn3.running_mean\", \"resnet.6.1.bn3.running_var\", \"resnet.6.1.bn3.num_batches_tracked\", \"resnet.6.2.conv1.weight\", \"resnet.6.2.bn1.weight\", \"resnet.6.2.bn1.bias\", \"resnet.6.2.bn1.running_mean\", \"resnet.6.2.bn1.running_var\", \"resnet.6.2.bn1.num_batches_tracked\", \"resnet.6.2.conv2.weight\", \"resnet.6.2.bn2.weight\", \"resnet.6.2.bn2.bias\", \"resnet.6.2.bn2.running_mean\", \"resnet.6.2.bn2.running_var\", \"resnet.6.2.bn2.num_batches_tracked\", \"resnet.6.2.conv3.weight\", \"resnet.6.2.bn3.weight\", \"resnet.6.2.bn3.bias\", \"resnet.6.2.bn3.running_mean\", \"resnet.6.2.bn3.running_var\", \"resnet.6.2.bn3.num_batches_tracked\", \"resnet.6.3.conv1.weight\", \"resnet.6.3.bn1.weight\", \"resnet.6.3.bn1.bias\", \"resnet.6.3.bn1.running_mean\", \"resnet.6.3.bn1.running_var\", \"resnet.6.3.bn1.num_batches_tracked\", \"resnet.6.3.conv2.weight\", \"resnet.6.3.bn2.weight\", \"resnet.6.3.bn2.bias\", \"resnet.6.3.bn2.running_mean\", \"resnet.6.3.bn2.running_var\", \"resnet.6.3.bn2.num_batches_tracked\", \"resnet.6.3.conv3.weight\", \"resnet.6.3.bn3.weight\", \"resnet.6.3.bn3.bias\", \"resnet.6.3.bn3.running_mean\", \"resnet.6.3.bn3.running_var\", \"resnet.6.3.bn3.num_batches_tracked\", \"resnet.6.4.conv1.weight\", \"resnet.6.4.bn1.weight\", \"resnet.6.4.bn1.bias\", \"resnet.6.4.bn1.running_mean\", \"resnet.6.4.bn1.running_var\", \"resnet.6.4.bn1.num_batches_tracked\", \"resnet.6.4.conv2.weight\", \"resnet.6.4.bn2.weight\", \"resnet.6.4.bn2.bias\", \"resnet.6.4.bn2.running_mean\", \"resnet.6.4.bn2.running_var\", \"resnet.6.4.bn2.num_batches_tracked\", \"resnet.6.4.conv3.weight\", \"resnet.6.4.bn3.weight\", \"resnet.6.4.bn3.bias\", \"resnet.6.4.bn3.running_mean\", \"resnet.6.4.bn3.running_var\", \"resnet.6.4.bn3.num_batches_tracked\", \"resnet.6.5.conv1.weight\", \"resnet.6.5.bn1.weight\", \"resnet.6.5.bn1.bias\", \"resnet.6.5.bn1.running_mean\", \"resnet.6.5.bn1.running_var\", \"resnet.6.5.bn1.num_batches_tracked\", \"resnet.6.5.conv2.weight\", \"resnet.6.5.bn2.weight\", \"resnet.6.5.bn2.bias\", \"resnet.6.5.bn2.running_mean\", \"resnet.6.5.bn2.running_var\", \"resnet.6.5.bn2.num_batches_tracked\", \"resnet.6.5.conv3.weight\", \"resnet.6.5.bn3.weight\", \"resnet.6.5.bn3.bias\", \"resnet.6.5.bn3.running_mean\", \"resnet.6.5.bn3.running_var\", \"resnet.6.5.bn3.num_batches_tracked\", \"resnet.7.0.conv1.weight\", \"resnet.7.0.bn1.weight\", \"resnet.7.0.bn1.bias\", \"resnet.7.0.bn1.running_mean\", \"resnet.7.0.bn1.running_var\", \"resnet.7.0.bn1.num_batches_tracked\", \"resnet.7.0.conv2.weight\", \"resnet.7.0.bn2.weight\", \"resnet.7.0.bn2.bias\", \"resnet.7.0.bn2.running_mean\", \"resnet.7.0.bn2.running_var\", \"resnet.7.0.bn2.num_batches_tracked\", \"resnet.7.0.conv3.weight\", \"resnet.7.0.bn3.weight\", \"resnet.7.0.bn3.bias\", \"resnet.7.0.bn3.running_mean\", \"resnet.7.0.bn3.running_var\", \"resnet.7.0.bn3.num_batches_tracked\", \"resnet.7.0.downsample.0.weight\", \"resnet.7.0.downsample.1.weight\", \"resnet.7.0.downsample.1.bias\", \"resnet.7.0.downsample.1.running_mean\", \"resnet.7.0.downsample.1.running_var\", \"resnet.7.0.downsample.1.num_batches_tracked\", \"resnet.7.1.conv1.weight\", \"resnet.7.1.bn1.weight\", \"resnet.7.1.bn1.bias\", \"resnet.7.1.bn1.running_mean\", \"resnet.7.1.bn1.running_var\", \"resnet.7.1.bn1.num_batches_tracked\", \"resnet.7.1.conv2.weight\", \"resnet.7.1.bn2.weight\", \"resnet.7.1.bn2.bias\", \"resnet.7.1.bn2.running_mean\", \"resnet.7.1.bn2.running_var\", \"resnet.7.1.bn2.num_batches_tracked\", \"resnet.7.1.conv3.weight\", \"resnet.7.1.bn3.weight\", \"resnet.7.1.bn3.bias\", \"resnet.7.1.bn3.running_mean\", \"resnet.7.1.bn3.running_var\", \"resnet.7.1.bn3.num_batches_tracked\", \"resnet.7.2.conv1.weight\", \"resnet.7.2.bn1.weight\", \"resnet.7.2.bn1.bias\", \"resnet.7.2.bn1.running_mean\", \"resnet.7.2.bn1.running_var\", \"resnet.7.2.bn1.num_batches_tracked\", \"resnet.7.2.conv2.weight\", \"resnet.7.2.bn2.weight\", \"resnet.7.2.bn2.bias\", \"resnet.7.2.bn2.running_mean\", \"resnet.7.2.bn2.running_var\", \"resnet.7.2.bn2.num_batches_tracked\", \"resnet.7.2.conv3.weight\", \"resnet.7.2.bn3.weight\", \"resnet.7.2.bn3.bias\", \"resnet.7.2.bn3.running_mean\", \"resnet.7.2.bn3.running_var\", \"resnet.7.2.bn3.num_batches_tracked\". \n",
      "Initializing with random weights.\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://dfcf07b8da1455aa02.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://dfcf07b8da1455aa02.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://dfcf07b8da1455aa02.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "class FoodClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FoodClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=None)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    model = FoodClassifier(num_classes)\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            state_dict = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "            # Load state dict with strict=False to ignore non-matching keys\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Initializing with random weights.\")\n",
    "    else:\n",
    "        print(f\"Model file not found at {model_path}. Initializing with random weights.\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_path = 'food _classifier/food_classifier (1).pth'\n",
    "dataset_path = 'food _classifier/DATA_DIR'\n",
    "\n",
    "try:\n",
    "    ds = load_from_disk(dataset_path)\n",
    "    categories = ds['train'].features['label'].names\n",
    "    num_classes = len(categories)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Using default categories...\")\n",
    "    categories = [f\"Category_{i}\" for i in range(11)]\n",
    "    num_classes = len(categories)\n",
    "\n",
    "model = load_model(model_path, num_classes)\n",
    "\n",
    "def predict_image(image):\n",
    "    try:\n",
    "        if image is None:\n",
    "            return {\"Error\": \"No image provided\"}\n",
    "\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            image = np.array(image)\n",
    "\n",
    "        img = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        img_tensor = data_transforms(img).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        top5_prob, top5_catid = torch.topk(probabilities, min(5, len(categories)))\n",
    "\n",
    "        results = {categories[top5_catid[i].item()]: float(top5_prob[i]) for i in range(top5_prob.size(0))}\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict_image: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return {\"Error\": \"An error occurred\"}\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict_image,\n",
    "    inputs=gr.Image(),\n",
    "    outputs=gr.Label(num_top_classes=5),\n",
    "    title=\"Food Classification\",\n",
    "    description=\"Upload an image of food to classify it into one of the available categories.\",\n",
    "    examples=[\n",
    "        \"https://images.pexels.com/photos/39803/pexels-photo-39803.jpeg?cs=srgb&dl=apple-fruit-healthy-food-39803.jpg&fm=jpg\",\n",
    "        \"https://imgs.search.brave.com/G1O6-4aTqLI2WJ2H5iQjo2tGmFG4HxmL0g7LpBsX6xo/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly93d3cu/cmVjaXBldGluZWF0/cy5jb20vdGFjaHlv/bi8yMDE5LzA5L0Nv/b2stcmljZS1vbi1z/dG92ZV82LmpwZw\",\n",
    "        \"https://tse1.mm.bing.net/th?id=OIP.SvmpgPsIHYoHS08RfnQx1wHaJ6&pid=Api&P=0&h=180\",\n",
    "        \"https://imgs.search.brave.com/7zLAVx637-Ij9d_zCXh54wdd8sGovacvW-633wgdt24/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9tZWRp/YS5nZXR0eWltYWdl/cy5jb20vaWQvMTM5/MzQwNDg5My9waG90/by90ZXJpeWFraS1z/aHJpbXAtd2l0aC1y/YW1lbi1ub29kbGVz/LmpwZz9zPTYxMng2/MTImdz0wJms9MjAm/Yz05TjNITjdSUlVi/RzVOQlZMd3lJM2ZV/SEJ5WERRVkk2RzNh/QkRVQlFCLVI4PQ\",\n",
    "        \"https://imgs.search.brave.com/rWUYUHBxlaQddySHlxGKEoLWBxsp6srd2Y4aCSpN55Q/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9tZWRp/YS5pc3RvY2twaG90/by5jb20vaWQvNTMx/NDY0MzY2L3Bob3Rv/L2JlZWYtc3RlYWtz/LW9uLXRoZS1ncmls/bC5qcGc_cz02MTJ4/NjEyJnc9MCZrPTIw/JmM9Z1A1VmlHbkow/OFlUelh0aFNPTUt6/WkVNcXRBYmNNMmpo/RWFvMDlXMWtBUT0\"\n",
    "        \n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "iface.launch(share=True, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://c30f2937f9e110cce5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c30f2937f9e110cce5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7861 <> https://c30f2937f9e110cce5.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "\n",
    "class FoodClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, lstm_hidden_size=256, lstm_num_layers=2):\n",
    "        super(FoodClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(weights=None)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])  # Remove the last FC layer\n",
    "        \n",
    "        self.lstm = nn.LSTM(2048, lstm_hidden_size, lstm_num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.size()\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(batch_size, 1, -1)  # Reshape for LSTM input\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.fc(lstm_out[:, -1, :])  # Use the last LSTM output\n",
    "        return x\n",
    "\n",
    "def load_model(model_path, num_classes):\n",
    "    model = FoodClassifier(num_classes)\n",
    "    if os.path.exists(model_path):\n",
    "        try:\n",
    "            state_dict = torch.load(model_path, map_location=torch.device('cpu'), weights_only=True)\n",
    "            model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Initializing with random weights.\")\n",
    "    else:\n",
    "        print(f\"Model file not found at {model_path}. Initializing with random weights.\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "model_path = 'food _classifier/food_classifier_cnn_lstm.pth'\n",
    "dataset_path = 'food _classifier/DATA_DIR'\n",
    "\n",
    "try:\n",
    "    ds = load_from_disk(dataset_path)\n",
    "    categories = ds['train'].features['label'].names\n",
    "    num_classes = len(categories)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Using default categories...\")\n",
    "    categories = [f\"Category_{i}\" for i in range(11)]\n",
    "    num_classes = len(categories)\n",
    "\n",
    "model = load_model(model_path, num_classes)\n",
    "\n",
    "def predict_image(image):\n",
    "    try:\n",
    "        if image is None:\n",
    "            return {\"Error\": \"No image provided\"}\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            image = np.array(image)\n",
    "        img = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "        img_tensor = data_transforms(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        top5_prob, top5_catid = torch.topk(probabilities, min(5, len(categories)))\n",
    "        results = {categories[top5_catid[i].item()]: float(top5_prob[i]) for i in range(top5_prob.size(0))}\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict_image: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return {\"Error\": \"An error occurred\"}\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict_image,\n",
    "    inputs=gr.Image(),\n",
    "    outputs=gr.Label(num_top_classes=5),\n",
    "    title=\"Food Classification\",\n",
    "    description=\"Upload an image of food to classify it into one of the available categories.\",\n",
    "    examples=[\n",
    "        \"https://images.pexels.com/photos/39803/pexels-photo-39803.jpeg?cs=srgb&dl=apple-fruit-healthy-food-39803.jpg&fm=jpg\",\n",
    "        \"https://tse1.mm.bing.net/th?id=OIP.SvmpgPsIHYoHS08RfnQx1wHaJ6&pid=Api&P=0&h=180\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "iface.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
